{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRG7824D/S2GW0KKnebGst",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arzooilistan/task1/blob/main/Untitled16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYrfgCykz7Mb",
        "outputId": "d69b2a6e-4b98-42b2-de8b-0df9f940faee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ---------- 1) Install required packages (run once) ----------\n",
        "!pip install -q transformers datasets evaluate sentencepiece accelerate gradio\n",
        "\n",
        "# ---------- 2) Imports ----------\n",
        "import os\n",
        "from datasets import load_dataset, ClassLabel\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
        "                          DataCollatorWithPadding, TrainingArguments, Trainer)\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# ---------- 3) Load AG News dataset ----------\n",
        "# Hugging Face Datasets provides AG News with split train/test\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "# dataset: dict with splits 'train' and 'test'\n",
        "print(dataset)\n",
        "\n",
        "# ---------- 4) Prepare tokenizer and model ----------\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# map labels -> ensure ClassLabel type (should already be)\n",
        "# Check labels\n",
        "print(\"Label feature:\", dataset[\"train\"].features[\"label\"])\n",
        "\n",
        "num_labels = len(dataset[\"train\"].features[\"label\"].names) if isinstance(dataset[\"train\"].features[\"label\"], ClassLabel) else len(set(dataset[\"train\"][\"label\"]))\n",
        "print(\"num_labels =\", num_labels)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
        "\n",
        "# ---------- 5) Tokenization / Preprocessing ----------\n",
        "max_length = 128\n",
        "\n",
        "def preprocess_fn(examples):\n",
        "    # AG News: fields 'text' (some splits have title+description) — dataset has 'text' field\n",
        "    # For safety, handle both 'text' or 'title'+'description'\n",
        "    if \"text\" in examples:\n",
        "        texts = examples[\"text\"]\n",
        "    else:\n",
        "        # fallback\n",
        "        texts = [ (t + \" \" + d) if (t and d) else (t or d or \"\") for t, d in zip(examples.get(\"title\", []), examples.get(\"description\", [])) ]\n",
        "    return tokenizer(texts, padding=False, truncation=True, max_length=max_length)\n",
        "\n",
        "# Apply tokenization (batched)\n",
        "tokenized = dataset.map(preprocess_fn, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "print(tokenized)\n",
        "\n",
        "# ---------- 6) Train/validation split (optional) ----------\n",
        "# We'll create a small validation split from train (say 5% or 10%) if not provided\n",
        "split = tokenized[\"train\"].train_test_split(test_size=0.05, seed=42)\n",
        "train_dataset = split[\"train\"]\n",
        "eval_dataset = split[\"test\"]\n",
        "test_dataset = tokenized[\"test\"]\n",
        "\n",
        "print(\"Train size:\", len(train_dataset), \"Eval size:\", len(eval_dataset), \"Test size:\", len(test_dataset))\n",
        "\n",
        "# ---------- 7) Data collator ----------\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# ---------- 8) Metrics ----------\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1_macro = f1_score(labels, preds, average=\"macro\")\n",
        "    f1_micro = f1_score(labels, preds, average=\"micro\")\n",
        "    precision = precision_score(labels, preds, average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(labels, preds, average=\"macro\", zero_division=0)\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"f1_macro\": f1_macro,\n",
        "        \"f1_micro\": f1_micro,\n",
        "        \"precision_macro\": precision,\n",
        "        \"recall_macro\": recall\n",
        "    }\n",
        "\n",
        "# ---------- 9) Training arguments ----------\n",
        "output_dir = \"./bert-agnews\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    evaluation_strategy=\"steps\",        # evaluate every `eval_steps`\n",
        "    eval_steps=500,                     # adjust depending on dataset size / batch size\n",
        "    per_device_train_batch_size=16,     # reduce if OOM\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=2,                 # tune as needed (2-3 is common baseline)\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    fp16=torch.cuda.is_available(),     # use mixed precision if GPU\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "# ---------- 10) Trainer ----------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# ---------- 11) Train ----------\n",
        "# Warning: training may take ~minutes to hours depending on GPU/time. Adjust epochs/batch size as needed.\n",
        "trainer.train()\n",
        "\n",
        "# ---------- 12) Evaluate on test set ----------\n",
        "print(\"Evaluating on TEST set...\")\n",
        "test_metrics = trainer.evaluate(test_dataset)\n",
        "print(test_metrics)\n",
        "\n",
        "# ---------- 13) Save model & tokenizer ----------\n",
        "model_save_path = \"./bert-agnews-final\"\n",
        "trainer.model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "print(\"Model + tokenizer saved to\", model_save_path)\n",
        "\n",
        "# ---------- 14) Quick inference helper ----------\n",
        "label_names = dataset[\"train\"].features[\"label\"].names\n",
        "print(\"Label names:\", label_names)\n",
        "\n",
        "from transformers import pipeline\n",
        "classif = pipeline(\"text-classification\", model=model_save_path, tokenizer=model_save_path, return_all_scores=False, device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "def predict_label(text):\n",
        "    res = classif(text, truncation=True, max_length=max_length)\n",
        "    # pipeline returns [{'label': 'LABEL_0', 'score': 0.98}] or actual label name depending on saved model config\n",
        "    return res\n",
        "\n",
        "# Example:\n",
        "print(predict_label(\"Apple releases new iPhone features in latest update\"))\n",
        "print(predict_label(\"Stock markets tumble amid global economic uncertainty\"))\n",
        "\n",
        "# ---------- 15) Lightweight interactive demo using Gradio ----------\n",
        "import gradio as gr\n",
        "\n",
        "def classify_text(text):\n",
        "    out = predict_label(text)\n",
        "    # Format nicely\n",
        "    if isinstance(out, list):\n",
        "        # pipeline returns list of dicts\n",
        "        label = out[0][\"label\"]\n",
        "        score = out[0][\"score\"]\n",
        "    else:\n",
        "        label = out[\"label\"]\n",
        "        score = out[\"score\"]\n",
        "    # Convert label to readable (if MODEL produced 'LABEL_i')\n",
        "    try:\n",
        "        # If label like 'LABEL_2' map to label_names\n",
        "        if label.startswith(\"LABEL_\"):\n",
        "            idx = int(label.split(\"_\")[-1])\n",
        "            label_readable = label_names[idx]\n",
        "        else:\n",
        "            label_readable = label\n",
        "    except Exception:\n",
        "        label_readable = label\n",
        "    return f\"{label_readable} (score={score:.3f})\"\n",
        "\n",
        "title = \"AG News Topic Classifier (bert-base-uncased)\"\n",
        "description = \"Enter a news headline or short text. Model fine-tuned on AG News (4 classes).\"\n",
        "\n",
        "iface = gr.Interface(fn=classify_text,\n",
        "                     inputs=gr.Textbox(lines=3, placeholder=\"Type a news headline...\"),\n",
        "                     outputs=\"text\",\n",
        "                     title=title,\n",
        "                     description=description,\n",
        "                     allow_flagging=\"never\")\n",
        "\n",
        "# In Colab, set share=True to create a public link (optional)\n",
        "iface.launch(share=True)\n"
      ]
    }
  ]
}